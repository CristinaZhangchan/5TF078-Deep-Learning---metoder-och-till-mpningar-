{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SImpdnZ3x_Pm",
        "HTqC5wVfHuFm"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEeaOfHvBk_Q"
      },
      "source": [
        "# 5TF078 Deep Learning Course\n",
        "## Excercise 4 NLP - Word2Vec\n",
        "Created by Tomas Nordström, Umeå University\n",
        "\n",
        "Revisions:\n",
        "* 2023-12-03 Initial version with three different word2vec models (Builtin, Glove, GoogleNews) to be used from within GenSim /ToNo\n",
        "* 2023-12-06 Fix for gensim.downloader that now seems missing /Tomas\n",
        "* 2024-03-24 Updated tests for Kaggle. /Tomas\n",
        "* 2024-04-23 Included student calculation of similarity /Tomas\n",
        "* 2024-04-30 Fixed depreciated gensim glove2word2vec /Tomas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization"
      ],
      "metadata": {
        "id": "6t_XD9M13sMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "### Is this notebook running on Colab?\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "### Is this notebook running on Kaggle?\n",
        "# Fool Kaggle into making kaggle_secrets avaiable\n",
        "try:\n",
        "    import kaggle_secrets\n",
        "except ImportError as e:\n",
        "    pass\n",
        "# Now we can test for Kaggle\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules"
      ],
      "metadata": {
        "id": "l5LvngJGNfWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"KERAS_BACKEND\"] = \"jax\" # Also jax,pytorch for Keras 3.0\n",
        "\n",
        "# Import Keras/TF libraries\n",
        "\n",
        "import keras\n",
        "print('Keras version:', keras.__version__)\n",
        "\n",
        "import tensorflow as tf\n",
        "print('TensorFlow version:', tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj4cihqkNftR",
        "outputId": "afaed31c-e7dd-42b6-e09b-4c1e96452807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keras version: 3.5.0\n",
            "TensorFlow version: 2.17.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper libraries\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import urllib\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import gensim\n",
        "import gensim.downloader as gsapi\n",
        "\n",
        "# Matlab plotting\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "lZsRDD9kLn2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/questions/37748105/how-to-use-progressbar-module-with-urlretrieve#53643011\n",
        "import progressbar\n",
        "class MyProgressBar():\n",
        "    def __init__(self):\n",
        "        self.pbar = None\n",
        "\n",
        "    def __call__(self, block_num, block_size, total_size):\n",
        "        if not self.pbar:\n",
        "            self.pbar=progressbar.ProgressBar(maxval=total_size)\n",
        "            self.pbar.start()\n",
        "\n",
        "        downloaded = block_num * block_size\n",
        "        if downloaded < total_size:\n",
        "            self.pbar.update(downloaded)\n",
        "        else:\n",
        "            self.pbar.finish()"
      ],
      "metadata": {
        "id": "HTGKKOrxJaKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Based on the Gensim framework\n",
        "\n",
        "Gensim Docs: https://radimrehurek.com/gensim/\n",
        "\n",
        "There are many ways to download models or data for word2vec models:\n",
        "1. The Gensim Builtin models (where 'glove-twitter-100' downloads 387 MB file)\n",
        "2. One of the Glove models (downloads a 822 MB zip file).\n",
        "3. GoogleNews based model"
      ],
      "metadata": {
        "id": "M6eK_hUWl29O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select one of Builtin, Glove, GoogleNews models to use\n",
        "model_to_use            = 'GoogleNews'\n",
        "model_to_use_if_builtin = 'glove-twitter-100'"
      ],
      "metadata": {
        "id": "sylqrAD8ytdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up a word2vec model"
      ],
      "metadata": {
        "id": "uyN1_RZZ9_Fu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Look at what builtin Gensim models we can use\n",
        "\n",
        "Note that you need to select one out of the possible builtin models!"
      ],
      "metadata": {
        "id": "SImpdnZ3x_Pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://radimrehurek.com/gensim/models/word2vec.html\n",
        "\n",
        "# Show all available models in gensim-data\n",
        "print(list(gsapi.info()['models'].keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aan4kJiDl2Gs",
        "outputId": "eec45bf8-ba90-4115-e87c-32fca722db32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model_to_use == 'Builtin':\n",
        "  w2v = gsapi.load(model_to_use_if_builtin)"
      ],
      "metadata": {
        "id": "9eBto-pamOuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using GloVe"
      ],
      "metadata": {
        "id": "HTqC5wVfHuFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's download pre-trained GloVe embeddings (a 822 MB zip file).\n",
        "\n",
        "The archive contains text-encoded vectors of various sizes: 50-dimensional, 100-dimensional, 200-dimensional, 300-dimensional. We'll use the 100-D ones.\n",
        "\n"
      ],
      "metadata": {
        "id": "BL27KAmmHnk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a file to kerasdata, but first check if it already exist, now with progress bar\n",
        "DOWNLOADS_DIR = './kerasdata'\n",
        "os.makedirs(DOWNLOADS_DIR, exist_ok=True) # create dir if not exist\n",
        "\n",
        "if model_to_use == 'Glove':\n",
        "  url= 'https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip'\n",
        "  # Split on the rightmost / and take everything on the right side of that; up until '?'\n",
        "  name = url.rsplit('/', 1)[-1].split('?', 1)[0]\n",
        "  filename = os.path.join(DOWNLOADS_DIR, name)\n",
        "\n",
        "  # Download the file if it does not exist\n",
        "  if not os.path.isfile(filename):\n",
        "    print(f'Retrieving url: {url}', flush=True)\n",
        "    urllib.request.urlretrieve(url, filename, reporthook=MyProgressBar())\n",
        "  else:\n",
        "    print(f'Using local zip file: {filename}')\n",
        "\n",
        "  GLOVEDIR = os.path.join(DOWNLOADS_DIR, 'Glove')\n",
        "  path_to_glove_file =  os.path.join(GLOVEDIR, 'glove.6B.100d.txt')\n",
        "  if not os.path.isfile(path_to_glove_file):\n",
        "    ZipFile(filename).extractall(GLOVEDIR)\n",
        "\n",
        "  print(f'Using local glove file: {path_to_glove_file}')\n",
        "\n",
        "  # Now convert the Glove file to a gensim word2vec\n",
        "  # https://stackoverflow.com/questions/48743053/how-to-save-and-load-glove-models#51319383\n",
        "  GENSIMGLOVEFILE = os.path.join(GLOVEDIR,\"gensim_glove_vectors.txt\")\n",
        "\n",
        "  if not os.path.isfile(GENSIMGLOVEFILE):\n",
        "    # glove2word2vec(glove_input_file=path_to_glove_file, word2vec_output_file=GENSIMGLOVEFILE)\n",
        "    w2v = gensim.models.KeyedVectors.load_word2vec_format(path_to_glove_file, binary=False, no_header=True)\n",
        "  else:\n",
        "    w2v = gensim.models.KeyedVectors.load_word2vec_format(GENSIMGLOVEFILE, binary=False)\n"
      ],
      "metadata": {
        "id": "osnC1eKhJew3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using GoogleNews file"
      ],
      "metadata": {
        "id": "dGQ_Q68Uqcoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if model_to_use == 'GoogleNews':\n",
        "  # Download a file to kerasdata, but first check if it already exist, now with progress bar\n",
        "  DOWNLOADS_DIR = './kerasdata'\n",
        "  os.makedirs(DOWNLOADS_DIR, exist_ok=True) # create dir if not exist\n",
        "\n",
        "  url= 'https://git.ri.se/tomas.nordstrom/mldata/-/raw/main/GoogleNews-vectors-negative300.bin.gz?inline=false'\n",
        "  # Split on the rightmost / and take everything on the right side of that; up until '?'\n",
        "  name = url.rsplit('/', 1)[-1].split('?', 1)[0]\n",
        "  filename = os.path.join(DOWNLOADS_DIR, name)\n",
        "\n",
        "  # Download the file if it does not exist\n",
        "  if not os.path.isfile(filename):\n",
        "    print(f'Retrieving url: {url}', flush=True)\n",
        "    urllib.request.urlretrieve(url, filename, reporthook=MyProgressBar())\n",
        "  else:\n",
        "    print(f'Using local file: {filename}')\n",
        "\n",
        "  # Create the word2vec model\n",
        "  w2v = gensim.models.KeyedVectors.load_word2vec_format(filename, binary=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUZ2bpleqpra",
        "outputId": "a661feb6-abe2-4fb0-ecd6-d487e8bd1799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using local file: ./kerasdata/GoogleNews-vectors-negative300.bin.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments\n",
        "\n",
        "Check out https://radimrehurek.com/gensim/models/keyedvectors.html#what-can-i-do-with-word-vectors for examples what you can do with these vectors."
      ],
      "metadata": {
        "id": "1WjE4HHtp9xy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First check out the word2vec model\n",
        "print(f'Model name {model_to_use}')\n",
        "print(f'Total number of words: {len(w2v.key_to_index)}') # Totalt antal ord\n",
        "print(f'Some example words: {list(w2v.key_to_index)[:20]}')\n",
        "embedding_len = len(w2v[0])\n",
        "print(f'Embedding vector length: {embedding_len}')\n",
        "\n",
        "# The check out a vector\n",
        "word1 = \"cat\"\n",
        "print(f'\\nLooking at \"{word1}\"')\n",
        "cat_vector = w2v[word1]\n",
        "print(f'Vector: {cat_vector[:10]}...')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj5nxf1iv0Y4",
        "outputId": "eb013d16-438b-462d-d276-1ba47a5ba6b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model name GoogleNews\n",
            "Total number of words: 3000000\n",
            "Some example words: ['</s>', 'in', 'for', 'that', 'is', 'on', '##', 'The', 'with', 'said', 'was', 'the', 'at', 'not', 'as', 'it', 'be', 'from', 'by', 'are']\n",
            "Embedding vector length: 300\n",
            "\n",
            "Looking at \"cat\"\n",
            "Vector: [ 0.0123291   0.20410156 -0.28515625  0.21679688  0.11816406  0.08300781\n",
            "  0.04980469 -0.00952148  0.22070312 -0.12597656]...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarity\n",
        "\n",
        "Now we want to find similarity between word vectors and need to define a [similarity measure](https://en.wikipedia.org/wiki/Similarity_measure). In this exercise we will use [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) as a similarity measure.\n",
        "\n",
        "Your task is now to define a similarity function, and then generate three word vectors for “cat”, “cut”, and “dog” and compare the similarity between them using your similarity function.\n"
      ],
      "metadata": {
        "id": "ohoI1-jgGd1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity(vec1,vec2):\n",
        "##################### TODO: YOUR CODE STARTS HERE #####################\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    magnitude_vec1 = np.linalg.norm(vec1)\n",
        "    magnitude_vec2 = np.linalg.norm(vec2)\n",
        "    # Compute cosine similarity\n",
        "    cosine_similarity = dot_product / (magnitude_vec1 * magnitude_vec2)\n",
        "    return cosine_similarity\n",
        "##################### TODO: YOUR CODE ENDS HERE #######################"
      ],
      "metadata": {
        "id": "AYSTfwsNHz9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_vector = w2v[\"cat\"]\n",
        "cut_vector = w2v[\"cut\"]\n",
        "dog_vector = w2v[\"dog\"]\n",
        "\n",
        "##################### TODO: YOUR CODE STARTS HERE #####################\n",
        "# Compare the similarity between \"cat”, “cut”, and “dog”\n",
        "# Compare similarities\n",
        "similarity_cat_cut = similarity(cat_vector, cut_vector)\n",
        "similarity_cat_dog = similarity(cat_vector, dog_vector)\n",
        "similarity_cut_dog = similarity(cut_vector, dog_vector)\n",
        "\n",
        "# Display the similarities\n",
        "print(f'Similarity between \"{\"cat\"}\" and \"{\"cut\"}\": {similarity_cat_cut:.4f}')\n",
        "print(f'Similarity between \"{\"cat\"}\" and \"{\"dog\"}\": {similarity_cat_dog:.4f}')\n",
        "print(f'Similarity between \"{\"cut\"}\" and \"{\"dog\"}\": {similarity_cut_dog:.4f}')\n",
        "\n",
        "##################### TODO: YOUR CODE ENDS HERE #######################"
      ],
      "metadata": {
        "id": "-CmmfLW2Ilfl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43f91f7c-e219-4892-f109-2468676c9e49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between \"cat\" and \"cut\": 0.0926\n",
            "Similarity between \"cat\" and \"dog\": 0.7609\n",
            "Similarity between \"cut\" and \"dog\": 0.0555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uppgift\n",
        "Skapa fler exempel och analysera dina resultat"
      ],
      "metadata": {
        "id": "yEcHb3gT70p2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##################### TODO: YOUR CODE STARTS HERE #####################\n",
        "car_vector = w2v[\"car\"]\n",
        "tree_vector = w2v[\"tree\"]\n",
        "animal_vector = w2v[\"animal\"]\n",
        "\n",
        "##################### TODO: YOUR CODE STARTS HERE #####################\n",
        "# Compare the similarity between \"cat”, “cut”, and “dog”\n",
        "# Compare similarities\n",
        "similarity_car_tree = similarity(car_vector, tree_vector)\n",
        "similarity_car_animal = similarity(car_vector, animal_vector)\n",
        "similarity_tree_animal = similarity(tree_vector, animal_vector)\n",
        "\n",
        "# Display the similarities\n",
        "print(f'Similarity between \"{\"car\"}\" and \"{\"tree\"}\": {similarity_car_tree:.4f}')\n",
        "print(f'Similarity between \"{\"car\"}\" and \"{\"animal\"}\": {similarity_car_animal:.4f}')\n",
        "print(f'Similarity between \"{\"tree\"}\" and \"{\"animal\"}\": {similarity_tree_animal:.4f}')\n",
        "\n",
        "\n",
        "##################### TODO: YOUR CODE ENDS HERE #######################"
      ],
      "metadata": {
        "id": "ax8oovHq9f2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7ca8b77-8370-4ab7-b87d-e478dacb4559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between \"car\" and \"tree\": 0.2711\n",
            "Similarity between \"car\" and \"animal\": 0.1680\n",
            "Similarity between \"tree\" and \"animal\": 0.2878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the example, three new words are added: \"car\", \"tree\", and \"animal\", and their vector representations are calculated.\n",
        "\n",
        "Semantically similar words (e.g., \"dog\" and \"cat\").\n",
        "Semantically unrelated words (e.g., \"car\" and \"tree\").\n",
        "Domain-specific words (e.g., words related to technology, animals, or nature).\n",
        "\n",
        "Similarity Calculation:\n",
        "The similarity function implemented earlier is used to compute the cosine similarity between these words.\n",
        "\n",
        "The similarity values between each pair of words are displayed to help understand how the model captures relationships between the words."
      ],
      "metadata": {
        "id": "HeRXHqtxg5kp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding most similar\n",
        "We are also interested in finding the n closest words to a certain vector.\n",
        "\n",
        "We could do this by calculating a similarity to all vectors in the word2vec dictionary/matrix and then sort according to similarity score and take the top n-values.\n",
        "\n",
        "This is clearly doable, but for this course round we will use the most_similar methods on our w2v object.\n",
        "To get the ten most similar words to computer we do:\n",
        "`sims = w2v.most_similar('computer', topn=10)`\n",
        "\n",
        "\n",
        "We sometimes want to do operations with the vectors to do “king”-”man”+”woman”, and to support that we can use the two parameters: positive and negative:\n",
        "`w2v.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)`\n"
      ],
      "metadata": {
        "id": "OvXVoc_pImBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classic example\n",
        "result = w2v.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "DFD1RC4chvWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################### TODO: YOUR CODE STARTS HERE #####################\n",
        "# Create more examples\n",
        "example1 = w2v.most_similar('science', topn = 1)\n",
        "print(example1)\n",
        "\n",
        "example2 = w2v.most_similar(positive=['Paris', 'Italy'], negative=['France'], topn=1)\n",
        "print(example2)\n",
        "\n",
        "example3 = w2v.most_similar('computer', topn=3)\n",
        "print(example3)\n",
        "\n",
        "example4 = w2v.most_similar(positive=['man', 'queen'], negative=['woman'], topn=1)\n",
        "print(example4)\n",
        "\n",
        "example5 = w2v.most_similar(positive=['dog', 'cat'], negative=['animal'], topn=1)\n",
        "print(example5)\n",
        "##################### TODO: YOUR CODE ENDS HERE #######################"
      ],
      "metadata": {
        "id": "vaaKXVPk9XXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uppgift\n",
        "Skapa fler \"räkneexempel\" och analysera dina resultat"
      ],
      "metadata": {
        "id": "jZIXkPAh7kcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## There is actually a builtin similarity method"
      ],
      "metadata": {
        "id": "tA0NJz_c8M03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# There is a built in similarity methods we can use instead of our own function\n",
        "word1 = 'cat'\n",
        "word2 = 'dog'\n",
        "print(f\"Similarity between {word1} and {word2} is {w2v.similarity(word1, word2)}\")\n",
        "\n",
        "word2 = 'cut'\n",
        "print(f\"Similarity between {word1} and {word2} is {w2v.similarity(word1, word2)}\")"
      ],
      "metadata": {
        "id": "tnr43iynhOhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uppgift\n",
        "Får du samma resultat med din likhetsfunktion som den i gensim inbyggda funktionen?"
      ],
      "metadata": {
        "id": "JHJRWxg48AWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. This confirms that our implementation is mathematically correct from wikipedia.\n",
        "\n",
        "2. Both methods calculate cosine similarity, which measures the angle between two vectors in a high-dimensional space. Since the formula is standard and deterministic, the results are the same.\n",
        "\n",
        "3. While the custom function demonstrates your understanding of cosine similarity, the built-in method is more convenient and optimized for production use, as it avoids redundant calculations and directly interfaces with the word embeddings."
      ],
      "metadata": {
        "id": "tA0Uem-PuWvQ"
      }
    }
  ]
}